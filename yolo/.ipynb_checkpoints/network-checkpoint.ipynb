{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_classes = 26+10+4   #Grossbuchstaben, Zahlen, {.:/-}\n",
    "anchors = [10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326]\n",
    "leaky_relu_value = 0.1\n",
    "number_anchors = np.size(anchors)/6\n",
    "features = int(number_anchors*(4+1+number_classes))\n",
    "dimension = 3\n",
    "ignore_threshold = 0.7\n",
    "\n",
    "\n",
    "class Yolo_v3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Yolo_v3, self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(dimension,32,3,stride=1,padding=1)\n",
    "        self.batch_1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(32,64,3,stride=2,padding=1)\n",
    "        self.batch_2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv_3 = nn.Conv2d(64,32,1,stride=1,padding=0)\n",
    "        self.batch_3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv_4 = nn.Conv2d(32,64,3,stride=1,padding=1)\n",
    "        self.batch_4 = nn.BatchNorm2d(64)\n",
    "\n",
    "\n",
    "        \n",
    "        self.conv_6 = nn.Conv2d(64,128,3,stride=2,padding=1)\n",
    "        self.batch_6 = nn.BatchNorm2d(128)\n",
    "            \n",
    "        self.conv_7 = nn.Conv2d(128,64,1,stride=1,padding=0)\n",
    "        self.batch_7 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv_8 = nn.Conv2d(64,128,3,stride=1,padding=1)\n",
    "        self.batch_8 = nn.BatchNorm2d(128)\n",
    " \n",
    "\n",
    "          \n",
    "        self.conv_10 = nn.Conv2d(128,64,1,stride=1,padding=0)\n",
    "        self.batch_10 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv_11 = nn.Conv2d(64,128,3,stride=1,padding=1)\n",
    "        self.batch_11 = nn.BatchNorm2d(128)\n",
    "          \n",
    "\n",
    "             \n",
    "        self.conv_13 = nn.Conv2d(128,256,3,stride=2,padding=1)\n",
    "        self.batch_13 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv_14 = nn.Conv2d(256,128,1,stride=1,padding=0)\n",
    "        self.batch_14 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv_15 = nn.Conv2d(128,256,3,stride=1,padding=1)\n",
    "        self.batch_15 = nn.BatchNorm2d(256)        \n",
    "        \n",
    "\n",
    "\n",
    "        self.conv_17 = nn.Conv2d(256,128,1,stride=1,padding=0)\n",
    "        self.batch_17 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv_18 = nn.Conv2d(128,256,3,stride=1,padding=1)\n",
    "        self.batch_18 = nn.BatchNorm2d(256)\n",
    "        \n",
    "       \n",
    "    \n",
    "        \n",
    "        self.conv_38 = nn.Conv2d(256,512,3,stride=2,padding=1)\n",
    "        self.batch_38 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv_39 = nn.Conv2d(512,256,1,stride=1,padding=0)\n",
    "        self.batch_39 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv_40 = nn.Conv2d(256,512,3,stride=1,padding=1)\n",
    "        self.batch_40 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_42 = nn.Conv2d(512,256,1,stride=1,padding=0)\n",
    "        self.batch_42 = nn.BatchNorm2d(256)        \n",
    "        \n",
    "        self.conv_43 = nn.Conv2d(256,512,3,stride=1,padding=1)\n",
    "        self.batch_43 = nn.BatchNorm2d(512)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.conv_63 = nn.Conv2d(512,1024,3,stride=2,padding=1)\n",
    "        self.batch_63 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.conv_64 = nn.Conv2d(1024,512,1,stride=1,padding=0)\n",
    "        self.batch_64 = nn.BatchNorm2d(512)        \n",
    "        \n",
    "        self.conv_65 = nn.Conv2d(512,1024,3,stride=1,padding=1)\n",
    "        self.batch_65 = nn.BatchNorm2d(1024)  \n",
    "        \n",
    "\n",
    "        \n",
    "        self.conv_67 = nn.Conv2d(1024,512,1,stride=1,padding=0)\n",
    "        self.batch_67 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv_68 = nn.Conv2d(512,1024,3,stride=1,padding=1)\n",
    "        self.batch_68 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_76 = nn.Conv2d(1024,512,1,stride=1,padding=0)\n",
    "        self.batch_76 = nn.BatchNorm2d(512)        \n",
    "        \n",
    "        self.conv_77 = nn.Conv2d(512,1024,3,stride=1,padding=1)\n",
    "        self.batch_77 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv_82 = nn.Conv2d(1024,features,1,stride=1,padding=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_85 = nn.Conv2d(512,256,1,stride=1,padding=0)\n",
    "        self.batch_85 = nn.BatchNorm2d(256)       \n",
    "\n",
    "        self.convT_86 = nn.ConvTranspose2d(256,256,3,stride=2,padding=1,output_padding=1)\n",
    "        self.batch_86 = nn.BatchNorm2d(256) \n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv_88 = nn.Conv2d(512,256,1,stride=1,padding=0)\n",
    "        self.batch_88 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv_89 = nn.Conv2d(256,512,3,stride=1,padding=1)\n",
    "        self.batch_89 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_94 = nn.Conv2d(512,features,1,stride=1,padding=0)\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        self.conv_97 = nn.Conv2d(256,128,1,stride=1,padding=0)\n",
    "        self.batch_97 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.convT_98 = nn.ConvTranspose2d(128,128,3,stride=2,padding=1,output_padding=1)\n",
    "        self.batch_98 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_100 = nn.Conv2d(256,128,1,stride=1,padding=0)\n",
    "        self.batch_100 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv_101 = nn.Conv2d(128,256,3,stride=1,padding=1)\n",
    "        self.batch_101 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_106 = nn.Conv2d(256,features,1,stride=1,padding=0)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        layer_1 = F.leaky_relu(self.batch_1(self.conv_1(inputs)),leaky_relu_value,True)\n",
    "        layer_2 = F.leaky_relu(self.batch_2(self.conv_2(layer_1)),leaky_relu_value,True)\n",
    "        layer_3 = F.leaky_relu(self.batch_3(self.conv_3(layer_2)),leaky_relu_value,True)\n",
    "        layer_4 = F.leaky_relu(self.batch_4(self.conv_4(layer_3)),leaky_relu_value,True)\n",
    "        layer_5 = layer_2 + layer_4\n",
    "        layer_6 = F.leaky_relu(self.batch_6(self.conv_6(layer_5)),leaky_relu_value,True)\n",
    "        layer_7 = F.leaky_relu(self.batch_7(self.conv_7(layer_6)),leaky_relu_value,True)\n",
    "        layer_8 = F.leaky_relu(self.batch_8(self.conv_8(layer_7)),leaky_relu_value,True)\n",
    "        layer_9 = layer_6 + layer_8\n",
    "        layer_10 = F.leaky_relu(self.batch_10(self.conv_10(layer_9)),leaky_relu_value,True)\n",
    "        layer_11 = F.leaky_relu(self.batch_11(self.conv_11(layer_10)),leaky_relu_value,True)\n",
    "        layer_12 = layer_9 + layer_11\n",
    "        layer_13 = F.leaky_relu(self.batch_13(self.conv_13(layer_12)),leaky_relu_value,True)\n",
    "        layer_14 = F.leaky_relu(self.batch_14(self.conv_14(layer_13)),leaky_relu_value,True)\n",
    "        layer_15 = F.leaky_relu(self.batch_15(self.conv_15(layer_14)),leaky_relu_value,True)\n",
    "        layer_16 = layer_13 + layer_15\n",
    "        layer_17 = F.leaky_relu(self.batch_17(self.conv_17(layer_16)),leaky_relu_value,True)\n",
    "        layer_18 = F.leaky_relu(self.batch_18(self.conv_18(layer_17)),leaky_relu_value,True)\n",
    "        layer_19 = layer_16 + layer_18      \n",
    "        layer_20 = F.leaky_relu(self.batch_17(self.conv_17(layer_19)),leaky_relu_value,True)\n",
    "        layer_21 = F.leaky_relu(self.batch_18(self.conv_18(layer_20)),leaky_relu_value,True)\n",
    "        layer_22 = layer_19 + layer_21\n",
    "        layer_23 = F.leaky_relu(self.batch_17(self.conv_17(layer_22)),leaky_relu_value,True)\n",
    "        layer_24 = F.leaky_relu(self.batch_18(self.conv_18(layer_23)),leaky_relu_value,True)\n",
    "        layer_25 = layer_22 + layer_24\n",
    "        layer_26 = F.leaky_relu(self.batch_17(self.conv_17(layer_25)),leaky_relu_value,True)\n",
    "        layer_27 = F.leaky_relu(self.batch_18(self.conv_18(layer_26)),leaky_relu_value,True)\n",
    "        layer_28 = layer_25 + layer_27\n",
    "        layer_29 = F.leaky_relu(self.batch_17(self.conv_17(layer_28)),leaky_relu_value,True)\n",
    "        layer_30 = F.leaky_relu(self.batch_18(self.conv_18(layer_29)),leaky_relu_value,True)\n",
    "        layer_31 = layer_28 + layer_30\n",
    "        layer_32 = F.leaky_relu(self.batch_17(self.conv_17(layer_31)),leaky_relu_value,True)\n",
    "        layer_33 = F.leaky_relu(self.batch_18(self.conv_18(layer_32)),leaky_relu_value,True)\n",
    "        layer_34 = layer_31 + layer_33\n",
    "        layer_35 = F.leaky_relu(self.batch_17(self.conv_17(layer_34)),leaky_relu_value,True)\n",
    "        layer_36 = F.leaky_relu(self.batch_18(self.conv_18(layer_35)),leaky_relu_value,True)\n",
    "        layer_37 = layer_34 + layer_36\n",
    "        layer_38 = F.leaky_relu(self.batch_38(self.conv_38(layer_37)),leaky_relu_value,True)\n",
    "        layer_39 = F.leaky_relu(self.batch_39(self.conv_39(layer_38)),leaky_relu_value,True)\n",
    "        layer_40 = F.leaky_relu(self.batch_40(self.conv_40(layer_39)),leaky_relu_value,True)\n",
    "        layer_41 = layer_38 + layer_40\n",
    "        layer_42 = F.leaky_relu(self.batch_42(self.conv_42(layer_41)),leaky_relu_value,True)\n",
    "        layer_43 = F.leaky_relu(self.batch_43(self.conv_43(layer_42)),leaky_relu_value,True)\n",
    "        layer_44 = layer_41 + layer_43\n",
    "        layer_45 = F.leaky_relu(self.batch_42(self.conv_42(layer_44)),leaky_relu_value,True)\n",
    "        layer_46 = F.leaky_relu(self.batch_43(self.conv_43(layer_45)),leaky_relu_value,True)\n",
    "        layer_47 = layer_44 + layer_46\n",
    "        layer_48 = F.leaky_relu(self.batch_42(self.conv_42(layer_47)),leaky_relu_value,True)\n",
    "        layer_49 = F.leaky_relu(self.batch_43(self.conv_43(layer_48)),leaky_relu_value,True)\n",
    "        layer_50 = layer_47 + layer_49\n",
    "        layer_51 = F.leaky_relu(self.batch_42(self.conv_42(layer_50)),leaky_relu_value,True)\n",
    "        layer_52 = F.leaky_relu(self.batch_43(self.conv_43(layer_51)),leaky_relu_value,True)\n",
    "        layer_53 = layer_50 + layer_52\n",
    "        layer_54 = F.leaky_relu(self.batch_42(self.conv_42(layer_53)),leaky_relu_value,True)\n",
    "        layer_55 = F.leaky_relu(self.batch_43(self.conv_43(layer_54)),leaky_relu_value,True)\n",
    "        layer_56 = layer_53 +layer_55\n",
    "        layer_57 = F.leaky_relu(self.batch_42(self.conv_42(layer_56)),leaky_relu_value,True)\n",
    "        layer_58 = F.leaky_relu(self.batch_43(self.conv_43(layer_57)),leaky_relu_value,True)\n",
    "        layer_59 = layer_56 + layer_58\n",
    "        layer_60 = F.leaky_relu(self.batch_42(self.conv_42(layer_59)),leaky_relu_value,True)\n",
    "        layer_61 = F.leaky_relu(self.batch_43(self.conv_43(layer_60)),leaky_relu_value,True)\n",
    "        layer_62 = layer_59 + layer_61\n",
    "        layer_63 = F.leaky_relu(self.batch_63(self.conv_63(layer_62)),leaky_relu_value,True)\n",
    "        layer_64 = F.leaky_relu(self.batch_64(self.conv_64(layer_63)),leaky_relu_value,True)\n",
    "        layer_65 = F.leaky_relu(self.batch_65(self.conv_65(layer_64)),leaky_relu_value,True)\n",
    "        layer_66 = layer_63 + layer_65\n",
    "        layer_67 = F.leaky_relu(self.batch_67(self.conv_67(layer_66)),leaky_relu_value,True)\n",
    "        layer_68 = F.leaky_relu(self.batch_68(self.conv_68(layer_67)),leaky_relu_value,True)\n",
    "        layer_69 = layer_66 + layer_68\n",
    "        layer_70 = F.leaky_relu(self.batch_67(self.conv_67(layer_69)),leaky_relu_value,True)\n",
    "        layer_71 = F.leaky_relu(self.batch_68(self.conv_68(layer_70)),leaky_relu_value,True)\n",
    "        layer_72 = layer_69 + layer_71\n",
    "        layer_73 = F.leaky_relu(self.batch_67(self.conv_67(layer_72)),leaky_relu_value,True)\n",
    "        layer_74 = F.leaky_relu(self.batch_68(self.conv_68(layer_73)),leaky_relu_value,True)\n",
    "        layer_75 = layer_72 + layer_74\n",
    "        layer_76 = F.leaky_relu(self.batch_76(self.conv_76(layer_75)),leaky_relu_value,True)\n",
    "        layer_77 = F.leaky_relu(self.batch_77(self.conv_77(layer_76)),leaky_relu_value,True)\n",
    "        layer_78 = F.leaky_relu(self.batch_76(self.conv_76(layer_77)),leaky_relu_value,True)\n",
    "        layer_79 = F.leaky_relu(self.batch_77(self.conv_77(layer_78)),leaky_relu_value,True)\n",
    "        layer_80 = F.leaky_relu(self.batch_76(self.conv_76(layer_79)),leaky_relu_value,True)\n",
    "        layer_81 = F.leaky_relu(self.batch_77(self.conv_77(layer_80)),leaky_relu_value,True)\n",
    "        layer_82 = self.conv_82(layer_81)\n",
    "        layer_83 = layer_82\n",
    "        layer_84 = layer_80   # 16x16\n",
    "        layer_85 = F.leaky_relu(self.batch_85(self.conv_85(layer_84)),leaky_relu_value,True)\n",
    "        layer_86 = F.leaky_relu(self.batch_86(self.convT_86(layer_85)),leaky_relu_value,True)\n",
    "        layer_87 = torch.cat((layer_60,layer_86),1)\n",
    "        layer_88 = F.leaky_relu(self.batch_88(self.conv_88(layer_87)),leaky_relu_value,True)\n",
    "        layer_89 = F.leaky_relu(self.batch_89(self.conv_89(layer_88)),leaky_relu_value,True)\n",
    "        layer_90 = F.leaky_relu(self.batch_88(self.conv_88(layer_89)),leaky_relu_value,True)\n",
    "        layer_91 = F.leaky_relu(self.batch_89(self.conv_89(layer_90)),leaky_relu_value,True)\n",
    "        layer_92 = F.leaky_relu(self.batch_88(self.conv_88(layer_91)),leaky_relu_value,True)\n",
    "        layer_93 = F.leaky_relu(self.batch_89(self.conv_89(layer_92)),leaky_relu_value,True)\n",
    "        layer_94 = self.conv_94(layer_93)\n",
    "        layer_95 = layer_94\n",
    "        layer_96 = layer_92   # 32x32\n",
    "        layer_97 = F.leaky_relu(self.batch_97(self.conv_97(layer_96)),leaky_relu_value,True)\n",
    "        layer_98 = F.leaky_relu(self.batch_98(self.convT_98(layer_97)),leaky_relu_value,True)\n",
    "        layer_99 = torch.cat((layer_35,layer_98),1)\n",
    "        layer_100 = F.leaky_relu(self.batch_100(self.conv_100(layer_99)),leaky_relu_value,True)\n",
    "        layer_101 = F.leaky_relu(self.batch_101(self.conv_101(layer_100)),leaky_relu_value,True)\n",
    "        layer_102 = F.leaky_relu(self.batch_100(self.conv_100(layer_101)),leaky_relu_value,True)\n",
    "        layer_103 = F.leaky_relu(self.batch_101(self.conv_101(layer_102)),leaky_relu_value,True)\n",
    "        layer_104 = F.leaky_relu(self.batch_100(self.conv_100(layer_103)),leaky_relu_value,True)\n",
    "        layer_105 = F.leaky_relu(self.batch_101(self.conv_101(layer_104)),leaky_relu_value,True)\n",
    "        layer_106 = self.conv_106(layer_105)\n",
    "        layer_107 = layer_106   # 64x64\n",
    "        return layer_83, layer_95, layer_106   # ...*3*(5+26+10+4) = 135\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yolo_v3(\n",
      "  (conv_1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batch_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_3): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batch_6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_7): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_10): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_11): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_13): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batch_13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_14): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_15): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_17): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_38): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batch_38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_39): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_39): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_40): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_42): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_42): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_43): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_43): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_63): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batch_63): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_64): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_64): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_65): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_65): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_67): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_67): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_68): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_68): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_76): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_76): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_77): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_77): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_82): Conv2d(1024, 135, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_85): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_85): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (convT_86): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (batch_86): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_88): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_88): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_89): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_89): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_94): Conv2d(512, 135, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_97): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_97): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (convT_98): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (batch_98): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_100): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_100): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_101): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_101): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_106): Conv2d(256, 135, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Yolo_v3()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 135, 16, 16]) torch.Size([1, 135, 32, 32]) torch.Size([1, 135, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 512, 512)\n",
    "output_layer_1, output_layer_2, output_layer_3 = net(x)\n",
    "print(output_layer_1.size(),output_layer_2.size(),output_layer_3.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer_dim(output_layer):\n",
    "    batch_number, outputs, y, x = output_layer.shape\n",
    "    return output_layer_1.view(batch_number, number_anchors, 5, outputs/5/number_anchors, y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 5, 9, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(output_layer_dim(output_layer_1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: yolo_filter_boxes\n",
    "\n",
    "def yolo_filter_boxes(output_layer, ignore_thresh  = 0.7):\n",
    "    \"\"\"Filters YOLO boxes by thresholding on object and class confidence.\n",
    "    \n",
    "    Arguments:\n",
    "    box_confidence -- tensor of shape (19, 19, 5, 1)   (1,anchor_number,1,y,x)\n",
    "    boxes -- tensor of shape (19, 19, 5, 4)   (1,anchor_number,4,y,x)\n",
    "    box_class_probs -- tensor of shape (19, 19, 5, 80)   (1,anchor_number,classes,y,x)\n",
    "    threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box\n",
    "    \n",
    "    Returns:\n",
    "    scores -- tensor of shape (None,), containing the class probability score for selected boxes\n",
    "    boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes\n",
    "    classes -- tensor of shape (None,), containing the index of the class detected by the selected boxes\n",
    "    \n",
    "    Note: \"None\" is here because you don't know the exact number of selected boxes, as it depends on the threshold. \n",
    "    For example, the actual output size of scores would be (10,) if there are 10 boxes.\n",
    "    \"\"\"\n",
    "    \n",
    "    box_confidence = output_layer\n",
    "    box_confidence[:,:,0<ingore_tresh,:,:] = 0\n",
    "    \n",
    "    # Step 1: Compute box scores\n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    box_scores = box_confidence[:,:,0,:,:,:] * box_class_probs[:,:,:,:,:,:] # 19x19x80   1,3,1,x,y * 1,3,\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Step 2: Find the box_classes thanks to the max box_scores, keep track of the corresponding score\n",
    "    ### START CODE HERE ### (≈ 2 lines)\n",
    "    box_classes = K.argmax(box_scores, axis = -1)  # 19x19x5x1  (1 class index)\n",
    "    box_class_scores = K.max(box_scores, axis = -1)  # 19x19x5x1 (1 class score)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Step 3: Create a filtering mask based on \"box_class_scores\" by using \"threshold\". The mask should have the\n",
    "    # same dimension as box_class_scores, and be True for the boxes you want to keep (with probability >= threshold)\n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    filtering_mask = box_class_scores >= threshold\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Step 4: Apply the mask to scores, boxes and classes\n",
    "    ### START CODE HERE ### (≈ 3 lines)\n",
    "    scores = tf.boolean_mask(box_class_scores, filtering_mask)\n",
    "    boxes = tf.boolean_mask(boxes, filtering_mask)\n",
    "    classes = tf.boolean_mask(box_classes, filtering_mask)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24])\n",
      "tensor([[-0.5010,  1.3612,  0.2611,  1.1507,  0.5662,  0.8486,  1.4634,\n",
      "         -0.7691,  0.6056,  1.1778,  1.2709,  0.5941, -1.3133, -3.1274,\n",
      "         -0.1573, -3.1905, -1.4463, -0.5437,  0.1982, -1.0954,  0.4507,\n",
      "         -1.0748,  0.3555,  1.3461]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,24)\n",
    "print(a.size())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 8])\n",
      "tensor([[[-0.5010,  1.3612,  0.2611,  1.1507,  0.5662,  0.8486,  1.4634,\n",
      "          -0.7691],\n",
      "         [ 0.6056,  1.1778,  1.2709,  0.5941, -1.3133, -3.1274, -0.1573,\n",
      "          -3.1905],\n",
      "         [-1.4463, -0.5437,  0.1982, -1.0954,  0.4507, -1.0748,  0.3555,\n",
      "           1.3461]]])\n"
     ]
    }
   ],
   "source": [
    "b = a.view(1,3,8)\n",
    "print(b.size())\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  torch.Size([1, 1, 12, 12])\n",
      "b:  torch.Size([1, 1, 6, 6])\n",
      "c:  torch.Size([1, 1, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,1,12,12)\n",
    "conv = nn.Conv2d(1,1,3, stride=2,padding=1)\n",
    "convT = nn.ConvTranspose2d(1,1,3,stride=2,padding=0)\n",
    "\n",
    "b = conv(a)\n",
    "c = convT(b)\n",
    "\n",
    "print(\"a: \", a.size())\n",
    "print(\"b: \", b.size())\n",
    "print(\"c: \", c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d:  torch.Size([1, 1, 6, 6])\n",
      "ConvTranspose2d:  torch.Size([1, 1, 24, 24])\n",
      "Conv2d und ConvTranspose2d:  torch.Size([1, 1, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1, 1, 12, 12)   # Batch, Anzahl channels, X,Y \n",
    "downsample = nn.Conv2d(1, 1, 3, stride=2, padding=1)\n",
    "upsample = nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1,output_padding=1)\n",
    "h = downsample(inputs)\n",
    "print('Conv2d: ', h.size())        # (1, 1, 6, 6)\n",
    "output = upsample(inputs)\n",
    "print('ConvTranspose2d: ', output.size())    # (1, 1, 12, 12)\n",
    "g = upsample(h)\n",
    "print('Conv2d und ConvTranspose2d: ', g.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 16, 16])\n",
      "torch.Size([1, 3, 40, 16, 16])\n",
      "torch.Size([1, 3, 40, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,3,45,16,16)\n",
    "b = torch.randn(1,3,45,16,16)\n",
    "a_1 = a[:,:,0:1,:,:]\n",
    "b_1 = b[:,:,5:45,:,:]\n",
    "print(a_1.size())\n",
    "print(b_1.size())\n",
    "c = a_1 * b_1\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 1.2345, -0.2779,  0.4316],\n",
      "           [ 0.3223, -1.1076,  0.3231],\n",
      "           [-0.2033,  1.7143,  0.4102]],\n",
      "\n",
      "          [[ 0.5316,  2.0876,  1.4991],\n",
      "           [-0.5455,  0.4920,  0.7709],\n",
      "           [ 0.8501,  0.3011,  0.6388]],\n",
      "\n",
      "          [[-1.9508, -0.6496,  1.1798],\n",
      "           [ 0.4514, -1.2193,  1.2102],\n",
      "           [ 1.7304, -0.9347,  0.1015]],\n",
      "\n",
      "          [[ 0.2990, -0.8414,  0.4076],\n",
      "           [-0.5978,  1.2635,  0.6946],\n",
      "           [-0.0865, -0.5163, -0.9529]],\n",
      "\n",
      "          [[-0.0847,  1.5828,  1.1811],\n",
      "           [-0.5947, -1.5706, -2.2262],\n",
      "           [-0.5729, -0.9777,  0.6255]],\n",
      "\n",
      "          [[ 0.5807, -0.4617, -0.6386],\n",
      "           [ 0.0175,  0.1681,  0.1788],\n",
      "           [-1.6400, -0.3631, -0.0907]],\n",
      "\n",
      "          [[-1.7215,  0.7929, -0.9163],\n",
      "           [-2.4092, -1.3608,  0.2910],\n",
      "           [ 0.8383,  0.2695,  1.3681]],\n",
      "\n",
      "          [[-0.2305, -0.1474,  2.2767],\n",
      "           [-0.0317,  0.3175,  0.0082],\n",
      "           [ 0.3093,  0.3842,  0.0533]]],\n",
      "\n",
      "\n",
      "         [[[ 0.4074,  0.2507,  1.5099],\n",
      "           [-0.8576,  0.2067,  1.0246],\n",
      "           [-1.6050,  0.1048, -0.1877]],\n",
      "\n",
      "          [[-1.2618,  1.4451,  0.8647],\n",
      "           [ 0.8592,  0.8282, -0.3831],\n",
      "           [-2.4599,  1.5404,  0.6232]],\n",
      "\n",
      "          [[ 0.6911,  1.6042, -0.3000],\n",
      "           [-0.7441,  0.2554,  0.8841],\n",
      "           [-0.2645,  0.2935,  0.0547]],\n",
      "\n",
      "          [[ 2.0521,  0.9927,  0.6217],\n",
      "           [-0.7374, -0.0502,  0.4520],\n",
      "           [-1.7494,  0.0856, -0.8716]],\n",
      "\n",
      "          [[ 0.1058,  0.8664,  0.6809],\n",
      "           [-0.0357,  0.6706,  1.4326],\n",
      "           [-1.0690,  0.3759,  0.7948]],\n",
      "\n",
      "          [[-0.4770, -0.7205, -0.0288],\n",
      "           [-0.6370,  0.0768,  0.3300],\n",
      "           [ 2.0410,  1.0967, -0.3541]],\n",
      "\n",
      "          [[ 1.1563,  0.5256,  0.4171],\n",
      "           [ 0.6357, -0.1442,  0.9984],\n",
      "           [ 0.7795,  0.9759, -0.3037]],\n",
      "\n",
      "          [[ 1.4942,  0.5273, -0.3132],\n",
      "           [-0.7051, -0.9703, -0.5688],\n",
      "           [ 0.8877, -1.3740,  1.1249]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1122,  2.2027, -0.2399],\n",
      "           [ 0.9813,  0.9093, -1.2529],\n",
      "           [ 0.0177, -0.5096, -0.5188]],\n",
      "\n",
      "          [[ 1.9336, -0.4091, -0.5803],\n",
      "           [ 0.9044,  0.8093,  1.1166],\n",
      "           [-0.3278,  1.9723,  1.9078]],\n",
      "\n",
      "          [[-0.3830, -0.6556,  0.2402],\n",
      "           [ 1.6567,  1.8139,  1.2807],\n",
      "           [ 1.3174, -1.0659, -0.7585]],\n",
      "\n",
      "          [[ 0.4081, -0.4363, -1.2835],\n",
      "           [ 0.2810, -1.2645,  1.0353],\n",
      "           [ 0.3939, -0.0355,  1.2452]],\n",
      "\n",
      "          [[ 1.4155,  1.2021, -0.3178],\n",
      "           [-1.1339, -1.0205,  1.0630],\n",
      "           [ 0.4365,  1.0144,  0.4817]],\n",
      "\n",
      "          [[-1.3853,  1.0283,  1.7956],\n",
      "           [-0.1041,  0.3118,  0.4987],\n",
      "           [-1.6426,  1.2369,  0.0163]],\n",
      "\n",
      "          [[-1.5113,  0.0633,  0.0491],\n",
      "           [ 0.8710, -0.4836, -0.6543],\n",
      "           [-0.7288,  0.8388, -0.0114]],\n",
      "\n",
      "          [[ 0.4695,  0.3340, -0.4420],\n",
      "           [-1.7588, -1.3169, -0.2060],\n",
      "           [-1.4908, -0.2962, -0.9619]]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,3,8,3,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_output[:,:,0,:,:] => pc\n",
    "# layer_output[:,:,1,:,:] => bx\n",
    "# layer_output[:,:,2,:,:] => by\n",
    "# layer_output[:,:,3,:,:] => tx\n",
    "# layer_output[:,:,4,:,:] => ty\n",
    "# layer_output[:,:,5,:,:] => c1\n",
    "# layer_output[:,:,n,:,:] => cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_index(box_scores, ignore_threshold = 0.7):\n",
    "    class_confidence = torch.cat((box_scores[:,0,:,:,:], box_scores[:,1,:,:,:], box_scores[:,2,:,:,:]),1)\n",
    "    class_confidence_max, class_confidence_max_index = torch.max(class_confidence,1)\n",
    "    batch_size, tmp_number_classes, y, x = class_confidence.shape\n",
    "    filter_mask = torch.zeros(batch_size, tmp_number_classes, y, x)\n",
    "    for i in range(y):\n",
    "        for j in range(x):\n",
    "            position = class_confidence_max_index[0,i,j]\n",
    "            filter_mask[0,position,i,j] = 1\n",
    "            if class_confidence[0,position,i,j] < ignore_threshold:\n",
    "                filter_mask[0,position,i,j] = 0\n",
    "    return filter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_surpression(layer_output, ignore_threshold = 0.7):\n",
    "    box_scores = torch.mul(layer_output[:,:,5:,:,:], layer_output[:,:,0,:,:]) # confidence * class score\n",
    "    filter_mask = max_index(box_scores, ignore_threshold)\n",
    "    # print(box_scores)\n",
    "    class_max = torch.mul(box_scores, filter_mask)\n",
    "    return class_max, filter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "tensor([[[[[ 0.7169,  0.1283, -0.2757],\n",
      "           [ 0.0057, -0.1862,  0.0578],\n",
      "           [ 0.3333, -0.6225, -0.0372]],\n",
      "\n",
      "          [[-0.7013,  0.1988, -1.3835],\n",
      "           [ 2.0662, -0.2813,  0.2981],\n",
      "           [-1.3455,  0.0282, -0.2568]],\n",
      "\n",
      "          [[-0.0259, -0.3246, -0.5461],\n",
      "           [-0.0311,  0.2887, -0.0102],\n",
      "           [ 0.0055, -0.1958, -0.0276]]],\n",
      "\n",
      "\n",
      "         [[[-0.5888,  0.2002, -0.0124],\n",
      "           [-0.2053, -0.0851,  0.1066],\n",
      "           [-0.4149,  1.8801, -0.1452]],\n",
      "\n",
      "          [[ 0.4710,  0.1318,  0.6298],\n",
      "           [-0.5452, -0.0298,  1.0229],\n",
      "           [-1.2511,  0.1023,  0.0570]],\n",
      "\n",
      "          [[ 0.1677,  1.1614,  0.0751],\n",
      "           [-0.6919, -0.8823,  0.7126],\n",
      "           [ 0.0157,  0.7001, -0.5836]]],\n",
      "\n",
      "\n",
      "         [[[-1.7102, -0.2857,  0.7750],\n",
      "           [-0.0336, -0.3453,  0.1611],\n",
      "           [ 0.3339,  2.1204,  0.0067]],\n",
      "\n",
      "          [[-0.6157,  0.0159,  0.0742],\n",
      "           [-0.7470, -0.0999, -0.6704],\n",
      "           [ 1.1697,  0.0879,  0.0021]],\n",
      "\n",
      "          [[ 0.0527,  0.7356,  0.1060],\n",
      "           [-1.7259, -1.1975,  0.2580],\n",
      "           [-0.0263,  0.1510,  0.4991]]]]])\n",
      "tensor([[[[ 1.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 1.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  1.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  1.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  1.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  1.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 1.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(ignore_threshold)\n",
    "z = non_max_surpression(a,0.6)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,3,8,3,3)\n",
    "b = non_max_surpression(a,0.7)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 3, 3])\n",
      "tensor([[[[[ 0.1567,  0.2116,  0.4470],\n",
      "           [ 0.0388, -0.3436, -0.1389],\n",
      "           [-0.2684,  0.5723,  0.8842]],\n",
      "\n",
      "          [[-0.2455, -1.3099,  1.3137],\n",
      "           [ 0.2063, -0.9575,  0.0641],\n",
      "           [-0.0270, -0.1416,  0.0129]],\n",
      "\n",
      "          [[ 0.3346,  1.7525,  1.0157],\n",
      "           [-0.0269, -0.8652,  0.1784],\n",
      "           [-0.9548,  0.0169, -1.0450]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1808,  0.0564, -0.1441],\n",
      "           [-0.0912,  0.1771, -0.2231],\n",
      "           [ 1.4383,  0.1073,  1.8358]],\n",
      "\n",
      "          [[ 0.7480,  1.0265,  1.0668],\n",
      "           [-0.0625, -0.9580,  0.9552],\n",
      "           [-0.0056, -0.0710, -0.0329]],\n",
      "\n",
      "          [[ 0.2037,  1.3560,  0.2751],\n",
      "           [ 0.0224, -0.0567,  0.0004],\n",
      "           [-0.3490, -0.5054,  0.4232]]],\n",
      "\n",
      "\n",
      "         [[[-0.0508,  0.1858,  0.8422],\n",
      "           [ 0.0781,  0.9442,  0.4005],\n",
      "           [ 0.3027,  0.5661,  0.3054]],\n",
      "\n",
      "          [[-0.7518,  1.2542,  0.4947],\n",
      "           [ 0.0373,  0.5722,  0.2297],\n",
      "           [ 0.0416, -0.0470,  0.1605]],\n",
      "\n",
      "          [[ 0.4000, -0.8028,  0.5752],\n",
      "           [-0.0157,  0.2656, -0.1310],\n",
      "           [-0.3902,  0.3429, -1.0473]]]]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.mul(a[:,:,5:,:,:], a[:,:,0,:,:])\n",
    "print(b.size())\n",
    "# b[b<0.1] = 0\n",
    "# b[b>=0.1] = 1\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   box_classes = K.argmax(box_scores, axis = -1)  # 19x19x5x1  (1 class index)\n",
    "#  box_class_scores = K.max(box_scores, axis = -1)  # 19x19x5x1 (1 class score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 0.1453,  0.3222, -0.8677],\n",
      "           [ 0.3942, -0.8767,  0.0186],\n",
      "           [-0.0429, -0.5691, -0.1548]],\n",
      "\n",
      "          [[ 1.4984, -0.6764, -0.3858],\n",
      "           [-0.1393, -0.5445, -0.3784],\n",
      "           [-0.0058,  0.9321,  1.5658]],\n",
      "\n",
      "          [[-0.0042, -0.9975,  0.1681],\n",
      "           [-0.7926,  1.1236,  0.0412],\n",
      "           [ 0.2972, -0.1403, -0.3645]],\n",
      "\n",
      "          [[-0.3765, -0.8055, -0.6821],\n",
      "           [-0.3558,  2.1598,  2.1103],\n",
      "           [ 0.1300, -0.3741,  0.0757]],\n",
      "\n",
      "          [[-2.5740, -0.5574, -0.7684],\n",
      "           [ 1.2945, -0.0641, -0.8182],\n",
      "           [-2.1312, -0.6864, -0.2527]],\n",
      "\n",
      "          [[-0.8003,  0.7683, -1.0649],\n",
      "           [ 1.8570, -0.8143, -0.5750],\n",
      "           [-1.3593, -0.9354, -0.1103]],\n",
      "\n",
      "          [[ 0.0672,  0.3824,  0.7212],\n",
      "           [ 2.9575, -1.2002, -1.2147],\n",
      "           [ 1.3152, -0.3910, -1.7388]],\n",
      "\n",
      "          [[-1.1646,  0.1989,  0.9756],\n",
      "           [-1.2275, -0.3666, -0.0395],\n",
      "           [ 0.1581, -1.8811, -0.5720]]],\n",
      "\n",
      "\n",
      "         [[[-0.6556,  0.6233,  1.3132],\n",
      "           [ 0.8257, -1.1156,  0.1855],\n",
      "           [-0.2171,  1.7124, -1.4058]],\n",
      "\n",
      "          [[-1.0323, -1.0515,  0.7021],\n",
      "           [-1.4100, -0.3313,  1.3009],\n",
      "           [ 1.1451, -0.8084, -0.0901]],\n",
      "\n",
      "          [[-0.4183, -1.9805,  1.1174],\n",
      "           [ 1.3895, -0.2220,  0.0268],\n",
      "           [ 0.8606, -0.3749,  1.3940]],\n",
      "\n",
      "          [[-0.3879, -0.9756,  0.1065],\n",
      "           [ 0.8104,  1.0158,  0.8491],\n",
      "           [-2.4179,  1.1393,  0.2565]],\n",
      "\n",
      "          [[ 0.0105, -1.9618, -0.4124],\n",
      "           [ 2.3191,  0.2469, -1.8353],\n",
      "           [-0.4983, -0.7022, -0.1076]],\n",
      "\n",
      "          [[-1.8747, -1.6715, -1.3859],\n",
      "           [-0.5295, -0.5642, -0.6924],\n",
      "           [-0.5394,  0.5514,  1.5575]],\n",
      "\n",
      "          [[ 2.5219,  0.6018,  0.9833],\n",
      "           [ 0.0880,  0.2821,  0.7378],\n",
      "           [-0.7471, -1.3530, -1.9507]],\n",
      "\n",
      "          [[ 0.7796, -0.1643, -0.4532],\n",
      "           [-1.5558, -0.2120,  0.2170],\n",
      "           [-1.5331, -0.8501,  1.0216]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6638, -0.6788,  0.4855],\n",
      "           [ 0.7685,  2.4586,  0.8653],\n",
      "           [ 1.3930, -0.8926,  0.7569]],\n",
      "\n",
      "          [[ 0.0404,  0.6929, -0.9280],\n",
      "           [ 1.5205,  1.0830,  0.1294],\n",
      "           [-0.6133,  0.8962,  0.8376]],\n",
      "\n",
      "          [[ 0.4329,  0.9391, -0.1937],\n",
      "           [-0.2987,  0.1866,  0.7659],\n",
      "           [ 0.0021,  0.9695,  0.3558]],\n",
      "\n",
      "          [[-0.4868,  0.3647, -0.0406],\n",
      "           [-1.7198, -3.5157,  1.2544],\n",
      "           [ 1.7959,  0.4936, -0.0069]],\n",
      "\n",
      "          [[ 0.2877,  1.0932,  1.6533],\n",
      "           [-0.4925, -1.0968,  0.2994],\n",
      "           [ 2.5744,  0.3388, -1.0890]],\n",
      "\n",
      "          [[-0.1025,  0.3199,  0.5447],\n",
      "           [-0.9048,  0.0820,  0.1154],\n",
      "           [-0.2631,  0.2083, -2.6952]],\n",
      "\n",
      "          [[-0.3947, -1.2871,  1.6479],\n",
      "           [-0.1356, -1.6607,  1.6340],\n",
      "           [ 1.0280,  1.4279, -0.7323]],\n",
      "\n",
      "          [[-1.3484,  1.7652, -0.8194],\n",
      "           [-0.1863, -0.3945,  0.2421],\n",
      "           [-0.3758, -0.3954,  0.3801]]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,3,8,3,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[-0.8149,  1.3050,  0.8988],\n",
      "           [-0.0289,  0.2133, -0.4548],\n",
      "           [ 0.3085, -0.9921,  0.4419]],\n",
      "\n",
      "          [[-0.4809,  0.8808,  0.8594],\n",
      "           [ 0.9602, -0.8180,  0.0453],\n",
      "           [ 0.4767, -1.9962, -0.0317]],\n",
      "\n",
      "          [[ 1.1382, -1.3587,  0.9941],\n",
      "           [ 0.8660,  2.5502, -0.3065],\n",
      "           [-0.4971, -0.0391, -1.3088]]],\n",
      "\n",
      "\n",
      "         [[[-0.9403,  0.3480, -0.2898],\n",
      "           [ 0.0680, -0.1100, -0.7302],\n",
      "           [-1.6528, -0.1860,  0.9175]],\n",
      "\n",
      "          [[ 1.4651, -0.6903,  0.6979],\n",
      "           [-0.2908, -0.8185,  0.6753],\n",
      "           [ 0.0986, -1.0006,  0.0808]],\n",
      "\n",
      "          [[ 0.6930, -1.0513,  0.2692],\n",
      "           [-0.7237,  0.1671, -0.0008],\n",
      "           [-0.1817,  1.1708,  0.5300]]],\n",
      "\n",
      "\n",
      "         [[[ 0.2645,  1.1454,  1.6934],\n",
      "           [-0.0582, -0.5861,  1.3110],\n",
      "           [-0.3479, -0.9814,  0.1526]],\n",
      "\n",
      "          [[-1.4726, -0.8433,  0.3236],\n",
      "           [ 0.1738,  0.4888,  0.1624],\n",
      "           [-0.7360, -0.6623, -0.3939]],\n",
      "\n",
      "          [[ 1.3607,  0.6224,  0.5630],\n",
      "           [ 0.5064, -0.7830,  0.2251],\n",
      "           [-0.2032, -0.7944, -1.3116]]]]])\n"
     ]
    }
   ],
   "source": [
    "c = a[:,:,5:,:,:]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 3, 3])\n",
      "torch.Size([1, 9, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "c1 = torch.cat((c[:,0,:,:,:], c[:,1,:,:,:], c[:,2,:,:,:]),1)\n",
    "print(c.size())\n",
    "print(c1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.8149,  1.3050,  0.8988],\n",
      "          [-0.0289,  0.2133, -0.4548],\n",
      "          [ 0.3085, -0.9921,  0.4419]],\n",
      "\n",
      "         [[-0.4809,  0.8808,  0.8594],\n",
      "          [ 0.9602, -0.8180,  0.0453],\n",
      "          [ 0.4767, -1.9962, -0.0317]],\n",
      "\n",
      "         [[ 1.1382, -1.3587,  0.9941],\n",
      "          [ 0.8660,  2.5502, -0.3065],\n",
      "          [-0.4971, -0.0391, -1.3088]],\n",
      "\n",
      "         [[-0.9403,  0.3480, -0.2898],\n",
      "          [ 0.0680, -0.1100, -0.7302],\n",
      "          [-1.6528, -0.1860,  0.9175]],\n",
      "\n",
      "         [[ 1.4651, -0.6903,  0.6979],\n",
      "          [-0.2908, -0.8185,  0.6753],\n",
      "          [ 0.0986, -1.0006,  0.0808]],\n",
      "\n",
      "         [[ 0.6930, -1.0513,  0.2692],\n",
      "          [-0.7237,  0.1671, -0.0008],\n",
      "          [-0.1817,  1.1708,  0.5300]],\n",
      "\n",
      "         [[ 0.2645,  1.1454,  1.6934],\n",
      "          [-0.0582, -0.5861,  1.3110],\n",
      "          [-0.3479, -0.9814,  0.1526]],\n",
      "\n",
      "         [[-1.4726, -0.8433,  0.3236],\n",
      "          [ 0.1738,  0.4888,  0.1624],\n",
      "          [-0.7360, -0.6623, -0.3939]],\n",
      "\n",
      "         [[ 1.3607,  0.6224,  0.5630],\n",
      "          [ 0.5064, -0.7830,  0.2251],\n",
      "          [-0.2032, -0.7944, -1.3116]]]])\n"
     ]
    }
   ],
   "source": [
    "print(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
