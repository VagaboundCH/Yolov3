{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_classes = 26+10+4   #Grossbuchstaben, Zahlen, {.:/-}\n",
    "anchors = [10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326]\n",
    "leaky_relu_value = 0.1\n",
    "number_anchors = np.size(anchors)/6\n",
    "features = int(number_anchors*(4+1+number_classes))\n",
    "dimension = 3\n",
    "ignore_threshold = 0.7\n",
    "batch_size = 1\n",
    "feature_map_size_1 = 16\n",
    "feature_map_size_2 = 32\n",
    "feature_map_size_3 = 64\n",
    "\n",
    "\n",
    "class Yolo_v3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Yolo_v3, self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(dimension,32,3,stride=1,padding=1)\n",
    "        self.batch_1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(32,64,3,stride=2,padding=1)\n",
    "        self.batch_2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv_3 = nn.Conv2d(64,32,1,stride=1,padding=0)\n",
    "        self.batch_3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv_4 = nn.Conv2d(32,64,3,stride=1,padding=1)\n",
    "        self.batch_4 = nn.BatchNorm2d(64)\n",
    "\n",
    "\n",
    "        \n",
    "        self.conv_6 = nn.Conv2d(64,128,3,stride=2,padding=1)\n",
    "        self.batch_6 = nn.BatchNorm2d(128)\n",
    "            \n",
    "        self.conv_7 = nn.Conv2d(128,64,1,stride=1,padding=0)\n",
    "        self.batch_7 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv_8 = nn.Conv2d(64,128,3,stride=1,padding=1)\n",
    "        self.batch_8 = nn.BatchNorm2d(128)\n",
    " \n",
    "\n",
    "          \n",
    "        self.conv_10 = nn.Conv2d(128,64,1,stride=1,padding=0)\n",
    "        self.batch_10 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv_11 = nn.Conv2d(64,128,3,stride=1,padding=1)\n",
    "        self.batch_11 = nn.BatchNorm2d(128)\n",
    "          \n",
    "\n",
    "             \n",
    "        self.conv_13 = nn.Conv2d(128,256,3,stride=2,padding=1)\n",
    "        self.batch_13 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv_14 = nn.Conv2d(256,128,1,stride=1,padding=0)\n",
    "        self.batch_14 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv_15 = nn.Conv2d(128,256,3,stride=1,padding=1)\n",
    "        self.batch_15 = nn.BatchNorm2d(256)        \n",
    "        \n",
    "\n",
    "\n",
    "        self.conv_17 = nn.Conv2d(256,128,1,stride=1,padding=0)\n",
    "        self.batch_17 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv_18 = nn.Conv2d(128,256,3,stride=1,padding=1)\n",
    "        self.batch_18 = nn.BatchNorm2d(256)\n",
    "        \n",
    "       \n",
    "    \n",
    "        \n",
    "        self.conv_38 = nn.Conv2d(256,512,3,stride=2,padding=1)\n",
    "        self.batch_38 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv_39 = nn.Conv2d(512,256,1,stride=1,padding=0)\n",
    "        self.batch_39 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv_40 = nn.Conv2d(256,512,3,stride=1,padding=1)\n",
    "        self.batch_40 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_42 = nn.Conv2d(512,256,1,stride=1,padding=0)\n",
    "        self.batch_42 = nn.BatchNorm2d(256)        \n",
    "        \n",
    "        self.conv_43 = nn.Conv2d(256,512,3,stride=1,padding=1)\n",
    "        self.batch_43 = nn.BatchNorm2d(512)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.conv_63 = nn.Conv2d(512,1024,3,stride=2,padding=1)\n",
    "        self.batch_63 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.conv_64 = nn.Conv2d(1024,512,1,stride=1,padding=0)\n",
    "        self.batch_64 = nn.BatchNorm2d(512)        \n",
    "        \n",
    "        self.conv_65 = nn.Conv2d(512,1024,3,stride=1,padding=1)\n",
    "        self.batch_65 = nn.BatchNorm2d(1024)  \n",
    "        \n",
    "\n",
    "        \n",
    "        self.conv_67 = nn.Conv2d(1024,512,1,stride=1,padding=0)\n",
    "        self.batch_67 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv_68 = nn.Conv2d(512,1024,3,stride=1,padding=1)\n",
    "        self.batch_68 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_76 = nn.Conv2d(1024,512,1,stride=1,padding=0)\n",
    "        self.batch_76 = nn.BatchNorm2d(512)        \n",
    "        \n",
    "        self.conv_77 = nn.Conv2d(512,1024,3,stride=1,padding=1)\n",
    "        self.batch_77 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv_82 = nn.Conv2d(1024,features,1,stride=1,padding=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_85 = nn.Conv2d(512,256,1,stride=1,padding=0)\n",
    "        self.batch_85 = nn.BatchNorm2d(256)       \n",
    "\n",
    "        self.convT_86 = nn.ConvTranspose2d(256,256,3,stride=2,padding=1,output_padding=1)\n",
    "        self.batch_86 = nn.BatchNorm2d(256) \n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv_88 = nn.Conv2d(512,256,1,stride=1,padding=0)\n",
    "        self.batch_88 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv_89 = nn.Conv2d(256,512,3,stride=1,padding=1)\n",
    "        self.batch_89 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_94 = nn.Conv2d(512,features,1,stride=1,padding=0)\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        self.conv_97 = nn.Conv2d(256,128,1,stride=1,padding=0)\n",
    "        self.batch_97 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.convT_98 = nn.ConvTranspose2d(128,128,3,stride=2,padding=1,output_padding=1)\n",
    "        self.batch_98 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_100 = nn.Conv2d(256,128,1,stride=1,padding=0)\n",
    "        self.batch_100 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv_101 = nn.Conv2d(128,256,3,stride=1,padding=1)\n",
    "        self.batch_101 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_106 = nn.Conv2d(256,features,1,stride=1,padding=0)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        layer_1 = F.leaky_relu(self.batch_1(self.conv_1(inputs)),leaky_relu_value,True)\n",
    "        layer_2 = F.leaky_relu(self.batch_2(self.conv_2(layer_1)),leaky_relu_value,True)\n",
    "        layer_3 = F.leaky_relu(self.batch_3(self.conv_3(layer_2)),leaky_relu_value,True)\n",
    "        layer_4 = F.leaky_relu(self.batch_4(self.conv_4(layer_3)),leaky_relu_value,True)\n",
    "        layer_5 = layer_2 + layer_4\n",
    "        layer_6 = F.leaky_relu(self.batch_6(self.conv_6(layer_5)),leaky_relu_value,True)\n",
    "        layer_7 = F.leaky_relu(self.batch_7(self.conv_7(layer_6)),leaky_relu_value,True)\n",
    "        layer_8 = F.leaky_relu(self.batch_8(self.conv_8(layer_7)),leaky_relu_value,True)\n",
    "        layer_9 = layer_6 + layer_8\n",
    "        layer_10 = F.leaky_relu(self.batch_10(self.conv_10(layer_9)),leaky_relu_value,True)\n",
    "        layer_11 = F.leaky_relu(self.batch_11(self.conv_11(layer_10)),leaky_relu_value,True)\n",
    "        layer_12 = layer_9 + layer_11\n",
    "        layer_13 = F.leaky_relu(self.batch_13(self.conv_13(layer_12)),leaky_relu_value,True)\n",
    "        layer_14 = F.leaky_relu(self.batch_14(self.conv_14(layer_13)),leaky_relu_value,True)\n",
    "        layer_15 = F.leaky_relu(self.batch_15(self.conv_15(layer_14)),leaky_relu_value,True)\n",
    "        layer_16 = layer_13 + layer_15\n",
    "        layer_17 = F.leaky_relu(self.batch_17(self.conv_17(layer_16)),leaky_relu_value,True)\n",
    "        layer_18 = F.leaky_relu(self.batch_18(self.conv_18(layer_17)),leaky_relu_value,True)\n",
    "        layer_19 = layer_16 + layer_18      \n",
    "        layer_20 = F.leaky_relu(self.batch_17(self.conv_17(layer_19)),leaky_relu_value,True)\n",
    "        layer_21 = F.leaky_relu(self.batch_18(self.conv_18(layer_20)),leaky_relu_value,True)\n",
    "        layer_22 = layer_19 + layer_21\n",
    "        layer_23 = F.leaky_relu(self.batch_17(self.conv_17(layer_22)),leaky_relu_value,True)\n",
    "        layer_24 = F.leaky_relu(self.batch_18(self.conv_18(layer_23)),leaky_relu_value,True)\n",
    "        layer_25 = layer_22 + layer_24\n",
    "        layer_26 = F.leaky_relu(self.batch_17(self.conv_17(layer_25)),leaky_relu_value,True)\n",
    "        layer_27 = F.leaky_relu(self.batch_18(self.conv_18(layer_26)),leaky_relu_value,True)\n",
    "        layer_28 = layer_25 + layer_27\n",
    "        layer_29 = F.leaky_relu(self.batch_17(self.conv_17(layer_28)),leaky_relu_value,True)\n",
    "        layer_30 = F.leaky_relu(self.batch_18(self.conv_18(layer_29)),leaky_relu_value,True)\n",
    "        layer_31 = layer_28 + layer_30\n",
    "        layer_32 = F.leaky_relu(self.batch_17(self.conv_17(layer_31)),leaky_relu_value,True)\n",
    "        layer_33 = F.leaky_relu(self.batch_18(self.conv_18(layer_32)),leaky_relu_value,True)\n",
    "        layer_34 = layer_31 + layer_33\n",
    "        layer_35 = F.leaky_relu(self.batch_17(self.conv_17(layer_34)),leaky_relu_value,True)\n",
    "        layer_36 = F.leaky_relu(self.batch_18(self.conv_18(layer_35)),leaky_relu_value,True)\n",
    "        layer_37 = layer_34 + layer_36\n",
    "        layer_38 = F.leaky_relu(self.batch_38(self.conv_38(layer_37)),leaky_relu_value,True)\n",
    "        layer_39 = F.leaky_relu(self.batch_39(self.conv_39(layer_38)),leaky_relu_value,True)\n",
    "        layer_40 = F.leaky_relu(self.batch_40(self.conv_40(layer_39)),leaky_relu_value,True)\n",
    "        layer_41 = layer_38 + layer_40\n",
    "        layer_42 = F.leaky_relu(self.batch_42(self.conv_42(layer_41)),leaky_relu_value,True)\n",
    "        layer_43 = F.leaky_relu(self.batch_43(self.conv_43(layer_42)),leaky_relu_value,True)\n",
    "        layer_44 = layer_41 + layer_43\n",
    "        layer_45 = F.leaky_relu(self.batch_42(self.conv_42(layer_44)),leaky_relu_value,True)\n",
    "        layer_46 = F.leaky_relu(self.batch_43(self.conv_43(layer_45)),leaky_relu_value,True)\n",
    "        layer_47 = layer_44 + layer_46\n",
    "        layer_48 = F.leaky_relu(self.batch_42(self.conv_42(layer_47)),leaky_relu_value,True)\n",
    "        layer_49 = F.leaky_relu(self.batch_43(self.conv_43(layer_48)),leaky_relu_value,True)\n",
    "        layer_50 = layer_47 + layer_49\n",
    "        layer_51 = F.leaky_relu(self.batch_42(self.conv_42(layer_50)),leaky_relu_value,True)\n",
    "        layer_52 = F.leaky_relu(self.batch_43(self.conv_43(layer_51)),leaky_relu_value,True)\n",
    "        layer_53 = layer_50 + layer_52\n",
    "        layer_54 = F.leaky_relu(self.batch_42(self.conv_42(layer_53)),leaky_relu_value,True)\n",
    "        layer_55 = F.leaky_relu(self.batch_43(self.conv_43(layer_54)),leaky_relu_value,True)\n",
    "        layer_56 = layer_53 +layer_55\n",
    "        layer_57 = F.leaky_relu(self.batch_42(self.conv_42(layer_56)),leaky_relu_value,True)\n",
    "        layer_58 = F.leaky_relu(self.batch_43(self.conv_43(layer_57)),leaky_relu_value,True)\n",
    "        layer_59 = layer_56 + layer_58\n",
    "        layer_60 = F.leaky_relu(self.batch_42(self.conv_42(layer_59)),leaky_relu_value,True)\n",
    "        layer_61 = F.leaky_relu(self.batch_43(self.conv_43(layer_60)),leaky_relu_value,True)\n",
    "        layer_62 = layer_59 + layer_61\n",
    "        layer_63 = F.leaky_relu(self.batch_63(self.conv_63(layer_62)),leaky_relu_value,True)\n",
    "        layer_64 = F.leaky_relu(self.batch_64(self.conv_64(layer_63)),leaky_relu_value,True)\n",
    "        layer_65 = F.leaky_relu(self.batch_65(self.conv_65(layer_64)),leaky_relu_value,True)\n",
    "        layer_66 = layer_63 + layer_65\n",
    "        layer_67 = F.leaky_relu(self.batch_67(self.conv_67(layer_66)),leaky_relu_value,True)\n",
    "        layer_68 = F.leaky_relu(self.batch_68(self.conv_68(layer_67)),leaky_relu_value,True)\n",
    "        layer_69 = layer_66 + layer_68\n",
    "        layer_70 = F.leaky_relu(self.batch_67(self.conv_67(layer_69)),leaky_relu_value,True)\n",
    "        layer_71 = F.leaky_relu(self.batch_68(self.conv_68(layer_70)),leaky_relu_value,True)\n",
    "        layer_72 = layer_69 + layer_71\n",
    "        layer_73 = F.leaky_relu(self.batch_67(self.conv_67(layer_72)),leaky_relu_value,True)\n",
    "        layer_74 = F.leaky_relu(self.batch_68(self.conv_68(layer_73)),leaky_relu_value,True)\n",
    "        layer_75 = layer_72 + layer_74\n",
    "        layer_76 = F.leaky_relu(self.batch_76(self.conv_76(layer_75)),leaky_relu_value,True)\n",
    "        layer_77 = F.leaky_relu(self.batch_77(self.conv_77(layer_76)),leaky_relu_value,True)\n",
    "        layer_78 = F.leaky_relu(self.batch_76(self.conv_76(layer_77)),leaky_relu_value,True)\n",
    "        layer_79 = F.leaky_relu(self.batch_77(self.conv_77(layer_78)),leaky_relu_value,True)\n",
    "        layer_80 = F.leaky_relu(self.batch_76(self.conv_76(layer_79)),leaky_relu_value,True)\n",
    "        layer_81 = F.leaky_relu(self.batch_77(self.conv_77(layer_80)),leaky_relu_value,True)\n",
    "        layer_82 = self.conv_82(layer_81)\n",
    "        layer_83 = layer_82\n",
    "        layer_84 = layer_80   # 16x16\n",
    "        layer_85 = F.leaky_relu(self.batch_85(self.conv_85(layer_84)),leaky_relu_value,True)\n",
    "        layer_86 = F.leaky_relu(self.batch_86(self.convT_86(layer_85)),leaky_relu_value,True)\n",
    "        layer_87 = torch.cat((layer_60,layer_86),1)\n",
    "        layer_88 = F.leaky_relu(self.batch_88(self.conv_88(layer_87)),leaky_relu_value,True)\n",
    "        layer_89 = F.leaky_relu(self.batch_89(self.conv_89(layer_88)),leaky_relu_value,True)\n",
    "        layer_90 = F.leaky_relu(self.batch_88(self.conv_88(layer_89)),leaky_relu_value,True)\n",
    "        layer_91 = F.leaky_relu(self.batch_89(self.conv_89(layer_90)),leaky_relu_value,True)\n",
    "        layer_92 = F.leaky_relu(self.batch_88(self.conv_88(layer_91)),leaky_relu_value,True)\n",
    "        layer_93 = F.leaky_relu(self.batch_89(self.conv_89(layer_92)),leaky_relu_value,True)\n",
    "        layer_94 = self.conv_94(layer_93)\n",
    "        layer_95 = layer_94\n",
    "        layer_96 = layer_92   # 32x32\n",
    "        layer_97 = F.leaky_relu(self.batch_97(self.conv_97(layer_96)),leaky_relu_value,True)\n",
    "        layer_98 = F.leaky_relu(self.batch_98(self.convT_98(layer_97)),leaky_relu_value,True)\n",
    "        layer_99 = torch.cat((layer_35,layer_98),1)\n",
    "        layer_100 = F.leaky_relu(self.batch_100(self.conv_100(layer_99)),leaky_relu_value,True)\n",
    "        layer_101 = F.leaky_relu(self.batch_101(self.conv_101(layer_100)),leaky_relu_value,True)\n",
    "        layer_102 = F.leaky_relu(self.batch_100(self.conv_100(layer_101)),leaky_relu_value,True)\n",
    "        layer_103 = F.leaky_relu(self.batch_101(self.conv_101(layer_102)),leaky_relu_value,True)\n",
    "        layer_104 = F.leaky_relu(self.batch_100(self.conv_100(layer_103)),leaky_relu_value,True)\n",
    "        layer_105 = F.leaky_relu(self.batch_101(self.conv_101(layer_104)),leaky_relu_value,True)\n",
    "        layer_106 = self.conv_106(layer_105)\n",
    "        layer_107 = layer_106   # 64x64\n",
    "        return layer_83, layer_95, layer_106   # ...*3*(5+26+10+4) = 135\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yolo_v3(\n",
      "  (conv_1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batch_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_3): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batch_6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_7): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_10): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_11): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_13): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batch_13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_14): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_15): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_17): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_38): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batch_38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_39): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_39): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_40): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_42): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_42): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_43): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_43): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_63): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batch_63): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_64): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_64): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_65): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_65): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_67): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_67): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_68): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_68): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_76): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_76): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_77): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_77): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_82): Conv2d(1024, 135, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_85): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_85): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (convT_86): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (batch_86): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_88): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_88): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_89): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_89): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_94): Conv2d(512, 135, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_97): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_97): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (convT_98): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (batch_98): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_100): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (batch_100): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_101): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_101): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_106): Conv2d(256, 135, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Yolo_v3()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 135, 16, 16]) torch.Size([1, 135, 32, 32]) torch.Size([1, 135, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 512, 512)\n",
    "output_layer_1, output_layer_2, output_layer_3 = net(x)\n",
    "print(output_layer_1.size(),output_layer_2.size(),output_layer_3.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_output[:,:,0,:,:] => pc\n",
    "# layer_output[:,:,1,:,:] => bx\n",
    "# layer_output[:,:,2,:,:] => by\n",
    "# layer_output[:,:,3,:,:] => tx\n",
    "# layer_output[:,:,4,:,:] => ty\n",
    "# layer_output[:,:,5,:,:] => c1\n",
    "# layer_output[:,:,n,:,:] => cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_index(class_scores, ignore_threshold = 0.7):\n",
    "    class_confidence = torch.cat((class_scores[:,0,:,:,:], class_scores[:,1,:,:,:], class_scores[:,2,:,:,:]),1)    # slice\n",
    "    class_confidence_max, class_confidence_max_index = torch.max(class_confidence,1)   # max & max_index\n",
    "    batch_size, tmp_number_classes, y, x = class_confidence.shape    # size for filter_mask \n",
    "    filter_mask = torch.zeros(batch_size, tmp_number_classes, y, x)\n",
    "    for i in range(y):\n",
    "        for j in range(x):\n",
    "            position = class_confidence_max_index[0,i,j]\n",
    "            filter_mask[0,position,i,j] = 1\n",
    "            if class_confidence[0,position,i,j] < ignore_threshold:    # zero if y < ignore_threshold\n",
    "                filter_mask[0,position,i,j] = 0    # set filter mask to zero if y < ignore_threshold\n",
    "    return filter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_surpression(layer_output, ignore_threshold = 0.7):\n",
    "    batch_size, features, y, x = layer_output.shape   # shape for slice\n",
    "    layer_output_sliced = layer_output.view(1,number_anchors,features/number_anchors,y,x)   # slice\n",
    "    class_scores = torch.mul(layer_output_sliced[:,:,5:,:,:], layer_output_sliced[:,:,0:1,:,:]) # confidence * class score\n",
    "    \n",
    "    filter_mask = max_index(class_scores, ignore_threshold)    # only max per cell\n",
    "    \n",
    "    class_scores_sliced = torch.cat((class_scores[:,0,:,:,:], class_scores[:,1,:,:,:], class_scores[:,2,:,:,:]),1) # slice for mul.\n",
    "    class_max = torch.mul(class_scores_sliced, filter_mask) \n",
    "    \n",
    "    box_scores = torch.cat((layer_output_sliced[:,0,0:5,:,:],layer_output_sliced[:,1,0:5,:,:],layer_output_sliced[:,2,0:5,:,:]),1)\n",
    "   \n",
    "    \n",
    "    return class_max, filter_mask, box_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1,45,3,3)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.7790,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  2.3127],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [ 2.9469, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 4.0222, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  1.4563],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.4454, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  2.1930, -0.0000],\n",
      "          [-0.0000,  0.0000,  2.5280]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.7524,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]]]])\n",
      "tensor([[[[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  1.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  1.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 1.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 1.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  1.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  1.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  1.,  0.],\n",
      "          [ 0.,  0.,  1.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 1.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]]]])\n",
      "tensor([[[[-0.1611,  0.4499, -1.2870],\n",
      "          [ 1.5472, -1.2128,  0.4436],\n",
      "          [ 0.0281,  0.3572, -0.1592]],\n",
      "\n",
      "         [[ 2.0594,  0.6659,  0.4681],\n",
      "          [-0.5317,  1.6134,  1.3100],\n",
      "          [ 0.1821, -0.5754,  0.0831]],\n",
      "\n",
      "         [[-0.2812, -0.2847, -1.0124],\n",
      "          [-1.1307,  0.5314, -0.0895],\n",
      "          [-0.7537, -1.1917,  1.2843]],\n",
      "\n",
      "         [[-0.1000, -1.1490,  1.5755],\n",
      "          [-0.4756,  0.5083,  0.7309],\n",
      "          [-0.9632, -0.3628,  0.4705]],\n",
      "\n",
      "         [[-0.0881, -0.1511,  0.3578],\n",
      "          [ 0.2697, -2.4203,  1.3925],\n",
      "          [-1.3895,  0.8461, -0.4987]],\n",
      "\n",
      "         [[-1.7459,  1.0957, -0.0274],\n",
      "          [-0.7363,  0.7744,  0.8363],\n",
      "          [-0.2568,  0.5237, -1.0598]],\n",
      "\n",
      "         [[-0.0548, -1.2054,  0.6876],\n",
      "          [-0.0737,  1.4459,  0.8278],\n",
      "          [-0.4790, -0.9603,  0.4601]],\n",
      "\n",
      "         [[ 0.9602, -1.4223,  0.4822],\n",
      "          [ 0.4787, -1.3014, -0.7783],\n",
      "          [-0.6073,  0.6385, -0.0530]],\n",
      "\n",
      "         [[ 1.2368, -1.1553, -0.4288],\n",
      "          [ 1.7202,  1.6943,  0.8112],\n",
      "          [-0.3266, -1.4665, -0.7206]],\n",
      "\n",
      "         [[ 0.2999,  1.7101, -0.3326],\n",
      "          [ 1.1239, -1.4252, -0.0104],\n",
      "          [ 0.0199, -0.6353, -0.3037]],\n",
      "\n",
      "         [[-0.9617,  1.8897,  0.0581],\n",
      "          [-0.3122, -0.8528,  0.6255],\n",
      "          [ 0.4382,  0.2115, -1.8415]],\n",
      "\n",
      "         [[ 0.6558,  2.0335,  0.1654],\n",
      "          [ 0.1487,  0.6914, -0.0550],\n",
      "          [-0.2400, -1.5900, -1.1259]],\n",
      "\n",
      "         [[-0.2597, -0.1808,  1.7873],\n",
      "          [ 0.2409,  1.9555, -0.3264],\n",
      "          [ 2.0085,  1.1965, -0.2515]],\n",
      "\n",
      "         [[ 1.4338, -0.0660,  0.2522],\n",
      "          [ 2.0571,  0.6139, -0.8398],\n",
      "          [-0.4325, -0.1827, -1.3708]],\n",
      "\n",
      "         [[-0.3112, -0.4125, -1.0892],\n",
      "          [-0.9771,  0.4836,  1.3068],\n",
      "          [ 0.9353,  1.1533, -1.7041]]]])\n"
     ]
    }
   ],
   "source": [
    "z0, z1, z2 = non_max_surpression(a,0.6)\n",
    "print(z0)\n",
    "print(z1)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output = torch.randn(1,48,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_scores:    torch.Size([1, 3, 11, 4, 4])\n",
      "nacher:     torch.Size([1, 11, 4, 4]) torch.Size([1, 11, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "batch_size, features, y, x = layer_output.shape   # shape for slice\n",
    "layer_output_sliced = layer_output.view(1,number_anchors,features/number_anchors,y,x)   # slice\n",
    "class_scores = torch.mul(layer_output_sliced[:,:,5:,:,:], layer_output_sliced[:,:,0:1,:,:]) # confidence * class score\n",
    "\n",
    "\n",
    "batch_size, tmp_number_classes, y, x = class_confidence.shape    # size for filter_mask \n",
    "filter_mask = torch.zeros(batch_size, tmp_number_classes, y, x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class_confidence = torch.cat((class_scores[:,0,:,:,:], class_scores[:,1,:,:,:], class_scores[:,2,:,:,:]),1)    # slice\n",
    "# class_confidence_max, class_confidence_max_index = torch.max(class_confidence,1)   # max & max_index\n",
    "# batch_size, tmp_number_classes, y, x = class_confidence.shape    # size for filter_mask \n",
    "# filter_mask = torch.zeros(batch_size, tmp_number_classes, y, x)\n",
    "\n",
    "# print('vorher:   ',class_confidence.size(),class_confidence_max.size(),class_confidence_max_index.size())\n",
    "\n",
    "print('class_scores:   ',class_scores.size())\n",
    "\n",
    "class_confidence_max_1, class_confidence_index_1 = torch.max(class_scores[:,0:1,:,:,:],1)\n",
    "class_confidence_max_2, class_confidence_index_2 = torch.max(class_scores[:,1:2,:,:,:],1)\n",
    "class_confidence_max_3, class_confidence_index_3 = torch.max(class_scores[:,2:3,:,:,:],1)\n",
    "class_confidence_max = torch.cat((class_confidence_max_1, class_confidence_max_2, class_confidence_max_3),1)\n",
    "class_confidence_index = torch.cat((class_confidence_index_1, class_confidence_index_2, class_confidence_index_3),1)\n",
    "\n",
    "print('nacher:    ',class_confidence_max_1.size(), class_confidence_index_1.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.2104, -0.3824,  0.4390,  0.4102],\n",
      "          [ 0.9781, -1.0226,  0.3044, -0.2071],\n",
      "          [ 1.2317, -0.0877,  0.3506, -0.0115],\n",
      "          [-0.7000,  1.1685, -0.2327,  0.5414]],\n",
      "\n",
      "         [[ 1.9857,  0.2910,  0.1142, -0.0463],\n",
      "          [ 0.9114, -1.6743, -0.4086, -0.0605],\n",
      "          [-2.7881, -0.0324,  0.2118,  0.8167],\n",
      "          [ 0.4082,  1.4417,  0.7643, -0.1745]],\n",
      "\n",
      "         [[-0.8430, -0.2323,  0.5513,  0.6136],\n",
      "          [-0.2409, -1.4124,  0.0636, -0.3503],\n",
      "          [ 2.5495, -0.1154,  0.1164, -0.0490],\n",
      "          [-0.8633,  0.0253,  0.1555, -1.3974]],\n",
      "\n",
      "         [[ 1.1936,  0.0451,  0.0555,  1.3909],\n",
      "          [-0.3032,  0.5130, -0.9062,  0.5266],\n",
      "          [ 1.7138,  0.0391,  0.0654,  0.1743],\n",
      "          [ 0.5822,  0.0006, -0.2068, -1.0644]],\n",
      "\n",
      "         [[ 0.5485,  0.1978,  0.0850, -0.9027],\n",
      "          [-0.6515, -0.9149,  0.3146,  0.5290],\n",
      "          [-1.6773,  0.0243,  0.3271, -0.3381],\n",
      "          [ 0.4137, -1.8187, -0.9816, -1.0371]],\n",
      "\n",
      "         [[ 1.7525,  0.0347,  0.0690,  0.2309],\n",
      "          [-0.6528, -0.6112, -0.1711, -1.1001],\n",
      "          [-0.4799,  0.0667, -0.5475, -0.3881],\n",
      "          [ 0.0869,  0.4867, -0.2793, -1.0122]],\n",
      "\n",
      "         [[-0.6097, -0.0173, -0.5873,  0.0036],\n",
      "          [ 1.8947,  0.0438,  0.4964,  0.5095],\n",
      "          [ 0.7853, -0.0426,  0.0959,  0.5453],\n",
      "          [ 2.1315,  0.8525,  0.8178,  0.8672]],\n",
      "\n",
      "         [[-0.6960,  0.0698,  0.0377, -1.3047],\n",
      "          [ 0.0581, -0.3534, -0.5128, -0.6389],\n",
      "          [ 1.7851,  0.0663, -0.7302, -0.5214],\n",
      "          [-1.0581,  0.3257, -0.3511, -0.6374]],\n",
      "\n",
      "         [[ 1.9253,  0.5419, -0.3252, -0.1116],\n",
      "          [ 0.4285,  0.4395, -0.4077,  0.1815],\n",
      "          [ 0.9070, -0.0222, -0.3510, -0.4116],\n",
      "          [-0.5865, -0.0773, -0.4608,  0.0458]],\n",
      "\n",
      "         [[-0.6844, -0.1878,  0.1772,  0.4499],\n",
      "          [-0.3025, -1.8485, -0.3386,  0.4534],\n",
      "          [ 0.4052, -0.0905,  0.2272, -0.8662],\n",
      "          [-1.4556,  1.3214,  1.5788,  1.2816]],\n",
      "\n",
      "         [[ 0.2551, -0.0663,  0.1253, -0.1206],\n",
      "          [ 0.3497, -0.2200, -0.6850,  0.2445],\n",
      "          [ 0.2215,  0.0397,  0.4675, -0.2540],\n",
      "          [ 0.2459,  0.7877,  1.0237, -1.1092]]]])\n"
     ]
    }
   ],
   "source": [
    "print(class_confidence_max_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1480083465576172\n"
     ]
    }
   ],
   "source": [
    "A = torch.randn(1,3,45,64,64)\n",
    "B = A.clone()\n",
    "start_time = time.time() \n",
    "A_max, A_index = torch.max(A,2,keepdim=True)\n",
    "A[A<A_max] = 0\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)\n",
    "# print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016000986099243164\n"
     ]
    }
   ],
   "source": [
    "# K = torch.rand(1,2,3,4,4)\n",
    "start_time = time.time()\n",
    "batch_size, number_anchors, number_classes, x, y = B.shape\n",
    "K_max, K_index = torch.max(B,2)# max finden\n",
    "batch = np.zeros(number_anchors*x*y) # batch bestimmen\n",
    "# print('batch: ', batch.size())\n",
    "x_index = np.arange(x).tolist() # tolist, weil numpy extend nicht kennt\n",
    "x_index.extend(x_index*(number_anchors*x-1)) # mit extend kann mehrmals die gleiche Liste angehÃ¤ngt werden\n",
    "# print('x_index: ', x_index)\n",
    "\n",
    "y_index = np.repeat(np.arange(y),y,axis=0).tolist() # reapet => 0123 wird 00112233\n",
    "y_index.extend(y_index*(number_anchors-1))\n",
    "# print('y_index: ', y_index)\n",
    "\n",
    "\n",
    "K_index_new = K_index.view(1,number_anchors*x*y)\n",
    "# print('K_index_new size: ', K_index_new)\n",
    "\n",
    "number_anchors_index = np.repeat(np.arange(number_anchors),x*y,axis=0)\n",
    "# print('number_anchorse: ', number_anchors_index)\n",
    "\n",
    "\n",
    "# print(K)\n",
    "# print(K_index)\n",
    "K_new = torch.zeros(batch_size, number_anchors, number_classes, x, y)\n",
    "\n",
    "K_new[batch, number_anchors_index, K_index_new, np.asarray(y_index), np.asarray(x_index)] = 1 # muss array sein keine Liste => np.asarray\n",
    "# print(K_new)\n",
    "K_max_finish = torch.mul(K_new,A)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 0, 3])\n",
    "b = np.zeros((3, 4))\n",
    "b[np.arange(3), a] = 1\n",
    "# https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_max_finish = torch.mul(K_new,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 0.0000,  0.9130,  0.0000,  0.0000],\n",
      "           [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "           [ 0.8451,  0.0000,  0.0000,  0.1389],\n",
      "           [ 0.0000,  1.5734,  0.0000,  0.0000]],\n",
      "\n",
      "          [[ 1.0116,  0.0000,  0.0000,  0.9630],\n",
      "           [ 0.0000,  0.0000,  0.0000,  0.8981],\n",
      "           [ 0.0000,  0.0000,  2.3796,  0.0000],\n",
      "           [ 0.0000,  0.0000,  0.5523,  0.0000]],\n",
      "\n",
      "          [[ 0.0000,  0.0000,  1.1603,  0.0000],\n",
      "           [ 0.3267,  1.2270,  0.9807,  0.0000],\n",
      "           [ 0.0000,  0.2357,  0.0000,  0.0000],\n",
      "           [ 0.6030,  0.0000,  0.0000,  0.3768]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "           [ 0.0000,  0.8396,  0.0000, -0.0856],\n",
      "           [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "           [ 0.0000, -0.1500,  0.0000,  0.0000]],\n",
      "\n",
      "          [[ 0.0000,  0.9494,  1.0051,  0.0000],\n",
      "           [ 0.7850,  0.0000,  0.7980,  0.0000],\n",
      "           [ 0.0000,  0.0000, -0.0001,  0.0000],\n",
      "           [ 0.4363,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "          [[-0.0835,  0.0000,  0.0000,  0.4915],\n",
      "           [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "           [ 1.3906, -0.1243,  0.0000,  0.8332],\n",
      "           [ 0.0000,  0.0000, -0.4223,  1.2040]]]]])\n"
     ]
    }
   ],
   "source": [
    "print(K_max_finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 0.0000,  0.9130,  0.0000,  0.0000],\n",
      "           [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "           [ 0.8451,  0.0000,  0.0000,  0.1389],\n",
      "           [ 0.0000,  1.5734,  0.0000,  0.0000]],\n",
      "\n",
      "          [[ 1.0116,  0.0000,  0.0000,  0.9630],\n",
      "           [ 0.0000,  0.0000,  0.0000,  0.8981],\n",
      "           [ 0.0000,  0.0000,  2.3796,  0.0000],\n",
      "           [ 0.0000,  0.0000,  0.5523,  0.0000]],\n",
      "\n",
      "          [[ 0.0000,  0.0000,  1.1603,  0.0000],\n",
      "           [ 0.3267,  1.2270,  0.9807,  0.0000],\n",
      "           [ 0.0000,  0.2357,  0.0000,  0.0000],\n",
      "           [ 0.6030,  0.0000,  0.0000,  0.3768]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "           [ 0.0000,  0.8396,  0.0000, -0.0856],\n",
      "           [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "           [ 0.0000, -0.1500,  0.0000,  0.0000]],\n",
      "\n",
      "          [[ 0.0000,  0.9494,  1.0051,  0.0000],\n",
      "           [ 0.7850,  0.0000,  0.7980,  0.0000],\n",
      "           [ 0.0000,  0.0000, -0.0001,  0.0000],\n",
      "           [ 0.4363,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "          [[-0.0835,  0.0000,  0.0000,  0.4915],\n",
      "           [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "           [ 1.3906, -0.1243,  0.0000,  0.8332],\n",
      "           [ 0.0000,  0.0000, -0.4223,  1.2040]]]]])\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1]],\n",
      "\n",
      "          [[ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1]],\n",
      "\n",
      "          [[ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1]]],\n",
      "\n",
      "\n",
      "         [[[ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1]],\n",
      "\n",
      "          [[ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1]],\n",
      "\n",
      "          [[ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1],\n",
      "           [ 1,  1,  1,  1]]]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "a = torch.eq(A, K_max_finish)\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
